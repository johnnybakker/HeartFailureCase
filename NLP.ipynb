{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Johnny\\AppData\\Roaming\\Python\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'array_record_module' from 'array_record.python' (C:\\Users\\Johnny\\AppData\\Roaming\\Python\\Python310\\site-packages\\array_record\\python\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[1;32mc:\\Studie\\Module - Artificial Intelligence\\HeartFailureCase\\NLP.ipynb Cell 1\u001B[0m line \u001B[0;36m5\n\u001B[0;32m      <a href='vscode-notebook-cell:/c%3A/Studie/Module%20-%20Artificial%20Intelligence/HeartFailureCase/NLP.ipynb#W0sZmlsZQ%3D%3D?line=2'>3</a>\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39mkeras\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mpreprocessing\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mtext\u001B[39;00m \u001B[39mimport\u001B[39;00m Tokenizer\n\u001B[0;32m      <a href='vscode-notebook-cell:/c%3A/Studie/Module%20-%20Artificial%20Intelligence/HeartFailureCase/NLP.ipynb#W0sZmlsZQ%3D%3D?line=3'>4</a>\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39mkeras\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mutils\u001B[39;00m \u001B[39mimport\u001B[39;00m pad_sequences\n\u001B[1;32m----> <a href='vscode-notebook-cell:/c%3A/Studie/Module%20-%20Artificial%20Intelligence/HeartFailureCase/NLP.ipynb#W0sZmlsZQ%3D%3D?line=4'>5</a>\u001B[0m \u001B[39mimport\u001B[39;00m \u001B[39mtensorflow_datasets\u001B[39;00m \u001B[39mas\u001B[39;00m \u001B[39mtfds\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow_datasets\\__init__.py:43\u001B[0m\n\u001B[0;32m     41\u001B[0m _TIMESTAMP_IMPORT_STARTS \u001B[39m=\u001B[39m time\u001B[39m.\u001B[39mtime()\n\u001B[0;32m     42\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39mabsl\u001B[39;00m \u001B[39mimport\u001B[39;00m logging\n\u001B[1;32m---> 43\u001B[0m \u001B[39mimport\u001B[39;00m \u001B[39mtensorflow_datasets\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mcore\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mlogging\u001B[39;00m \u001B[39mas\u001B[39;00m \u001B[39m_tfds_logging\u001B[39;00m\n\u001B[0;32m     44\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39mtensorflow_datasets\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mcore\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mlogging\u001B[39;00m \u001B[39mimport\u001B[39;00m call_metadata \u001B[39mas\u001B[39;00m _call_metadata\n\u001B[0;32m     46\u001B[0m _metadata \u001B[39m=\u001B[39m _call_metadata\u001B[39m.\u001B[39mCallMetadata()\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow_datasets\\core\\__init__.py:22\u001B[0m\n\u001B[0;32m     18\u001B[0m \u001B[39m# Allow to use `tfds.core.Path` in dataset implementation which seems more\u001B[39;00m\n\u001B[0;32m     19\u001B[0m \u001B[39m# natural than having to import a third party module.\u001B[39;00m\n\u001B[0;32m     20\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39metils\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mepath\u001B[39;00m \u001B[39mimport\u001B[39;00m Path\n\u001B[1;32m---> 22\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39mtensorflow_datasets\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mcore\u001B[39;00m \u001B[39mimport\u001B[39;00m community\n\u001B[0;32m     23\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39mtensorflow_datasets\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mcore\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mdataset_builder\u001B[39;00m \u001B[39mimport\u001B[39;00m BeamBasedBuilder\n\u001B[0;32m     24\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39mtensorflow_datasets\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mcore\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mdataset_builder\u001B[39;00m \u001B[39mimport\u001B[39;00m BuilderConfig\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow_datasets\\core\\community\\__init__.py:18\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[39m# coding=utf-8\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[39m# Copyright 2023 The TensorFlow Datasets Authors.\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[39m#\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[39m# See the License for the specific language governing permissions and\u001B[39;00m\n\u001B[0;32m     14\u001B[0m \u001B[39m# limitations under the License.\u001B[39;00m\n\u001B[0;32m     16\u001B[0m \u001B[39m\"\"\"Community dataset API.\"\"\"\u001B[39;00m\n\u001B[1;32m---> 18\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39mtensorflow_datasets\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mcore\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mcommunity\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mhuggingface_wrapper\u001B[39;00m \u001B[39mimport\u001B[39;00m mock_builtin_to_use_gfile\n\u001B[0;32m     19\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39mtensorflow_datasets\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mcore\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mcommunity\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mhuggingface_wrapper\u001B[39;00m \u001B[39mimport\u001B[39;00m mock_huggingface_import\n\u001B[0;32m     20\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39mtensorflow_datasets\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mcore\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mcommunity\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mload\u001B[39;00m \u001B[39mimport\u001B[39;00m builder_cls_from_module\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow_datasets\\core\\community\\huggingface_wrapper.py:31\u001B[0m\n\u001B[0;32m     28\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39munittest\u001B[39;00m \u001B[39mimport\u001B[39;00m mock\n\u001B[0;32m     30\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39metils\u001B[39;00m \u001B[39mimport\u001B[39;00m epath\n\u001B[1;32m---> 31\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39mtensorflow_datasets\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mcore\u001B[39;00m \u001B[39mimport\u001B[39;00m dataset_builder\n\u001B[0;32m     32\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39mtensorflow_datasets\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mcore\u001B[39;00m \u001B[39mimport\u001B[39;00m dataset_info\n\u001B[0;32m     33\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39mtensorflow_datasets\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mcore\u001B[39;00m \u001B[39mimport\u001B[39;00m download\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py:34\u001B[0m\n\u001B[0;32m     32\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39metils\u001B[39;00m \u001B[39mimport\u001B[39;00m epath\n\u001B[0;32m     33\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39mtensorflow_datasets\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mcore\u001B[39;00m \u001B[39mimport\u001B[39;00m constants\n\u001B[1;32m---> 34\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39mtensorflow_datasets\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mcore\u001B[39;00m \u001B[39mimport\u001B[39;00m dataset_info\n\u001B[0;32m     35\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39mtensorflow_datasets\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mcore\u001B[39;00m \u001B[39mimport\u001B[39;00m dataset_metadata\n\u001B[0;32m     36\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39mtensorflow_datasets\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mcore\u001B[39;00m \u001B[39mimport\u001B[39;00m decode\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow_datasets\\core\\dataset_info.py:47\u001B[0m\n\u001B[0;32m     45\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39metils\u001B[39;00m \u001B[39mimport\u001B[39;00m epath\n\u001B[0;32m     46\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39mtensorflow_datasets\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mcore\u001B[39;00m \u001B[39mimport\u001B[39;00m constants\n\u001B[1;32m---> 47\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39mtensorflow_datasets\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mcore\u001B[39;00m \u001B[39mimport\u001B[39;00m file_adapters\n\u001B[0;32m     48\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39mtensorflow_datasets\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mcore\u001B[39;00m \u001B[39mimport\u001B[39;00m lazy_imports_lib\n\u001B[0;32m     49\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39mtensorflow_datasets\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mcore\u001B[39;00m \u001B[39mimport\u001B[39;00m naming\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow_datasets\\core\\file_adapters.py:29\u001B[0m\n\u001B[0;32m     26\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39mtensorflow_datasets\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mcore\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mutils\u001B[39;00m \u001B[39mimport\u001B[39;00m type_utils\n\u001B[0;32m     27\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39mtensorflow_datasets\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mcore\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mutils\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mlazy_imports_utils\u001B[39;00m \u001B[39mimport\u001B[39;00m tensorflow \u001B[39mas\u001B[39;00m tf\n\u001B[1;32m---> 29\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39marray_record\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mpython\u001B[39;00m \u001B[39mimport\u001B[39;00m array_record_module\n\u001B[0;32m     31\u001B[0m ExamplePositions \u001B[39m=\u001B[39m List[Any]\n\u001B[0;32m     34\u001B[0m \u001B[39mclass\u001B[39;00m \u001B[39mFileFormat\u001B[39;00m(enum\u001B[39m.\u001B[39mEnum):\n",
      "\u001B[1;31mImportError\u001B[0m: cannot import name 'array_record_module' from 'array_record.python' (C:\\Users\\Johnny\\AppData\\Roaming\\Python\\Python310\\site-packages\\array_record\\python\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = [\n",
    "\t\"Mijn naam is Johnny\",\n",
    "\t\"Is jouw naam Pien\",\n",
    "\t\"Wat is jouw naam?\",\n",
    "\t\"Hoe gaat het?\",\n",
    "\t\"Het gaat goed\",\n",
    "\t\"Het gaat slecht\",\n",
    "\t\"Gaat het slecht?\",\n",
    "\t\"Gaat het goed?\",\n",
    "\t\"Hoe heet jij?\",\n",
    "\t\"Ik heet Johnny\",\n",
    "\t\"Mijn voornaam is Xanh\",\n",
    "\t\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tokenizer that we will use to extract the words of from the dataset\n",
    "tokenizer = Tokenizer(num_words = 100, oov_token=\"<OVV>\")\n",
    "\n",
    "# Provide the dataset\n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "\n",
    "# Grab the word index from the tokenizer\n",
    "# Example: { \"mijn\": 1, \"naam\": 2, \"is\": 3  }\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the dataset to sequence of numbers that can be translated back using the word_index\n",
    "sequences = tokenizer.texts_to_sequences(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the sequences so that every sequence contains the same amount of entries. \n",
    "# We need to pad the sequences because a neural network expects fixed size input.\n",
    "padded_seqs =  pad_sequences(sequences=sequences, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_sentences)\n",
    "print(word_index)\n",
    "print(sequences)\n",
    "print(padded_seqs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
